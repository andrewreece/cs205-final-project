{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import collections\n",
    "import time\n",
    "from math import gamma\n",
    "import cPickle as pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def getSubstring(substring,string):\n",
    "    matchs = []\n",
    "    for ii in range(0,len(string)-len(substring)):\n",
    "        if string[ii:ii+len(substring)] == substring:\n",
    "            matchs.append(ii)\n",
    "    if substring == '\"tweet\"':\n",
    "        assert len(matchs) > 0, \"no text\"\n",
    "        start = matchs[0]+8\n",
    "        while string[start:start+10] != '\"hashtags\"':\n",
    "            start = start + 1\n",
    "        return string[matchs[0]+8:start-1].lower()\n",
    "    elif substring == '\"hashtags\"':\n",
    "        for ii in matchs:\n",
    "            start = ii+11\n",
    "            end = ii+11\n",
    "            while string[end:end+3] != '}\\n':\n",
    "                end = end+1\n",
    "            hashtags = string[start:end]\n",
    "            if hashtags != '\"\"':\n",
    "                hashtags = np.copy(hashtags.split(','))\n",
    "                hashtags = np.copy(hashtags[:-1])\n",
    "                hashtags[0] = hashtags[0][1:]\n",
    "            else:\n",
    "                hashtags = \"\"\n",
    "        return np.array([ii.lower() for ii in hashtags])\n",
    "    elif substring == '\"timestamp\"':\n",
    "        start = matchs[0]+12\n",
    "        while string[start:start+11] != '\"screename\"':\n",
    "            start = start + 1\n",
    "        return string[matchs[0]+12:start-1]\n",
    "    elif substring == '\"screename\"':\n",
    "        start = matchs[0]+12\n",
    "        while string[start:start+10] != '\"location\"':\n",
    "            start = start + 1\n",
    "        return string[matchs[0]+12:start-1]\n",
    "    elif substring == '\"location\"':\n",
    "        start = matchs[0]+11\n",
    "        while string[start:start+5] != '\"geo\"':\n",
    "            start = start + 1\n",
    "        return string[matchs[0]+11:start-1]\n",
    "    else: # '\"geo\"'\n",
    "        start = matchs[0]+6\n",
    "        while string[start:start+7] != '\"tweet\"':\n",
    "            start = start + 1\n",
    "        return string[matchs[0]+6:start-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def preProcessWords(texts):\n",
    "    wordDocDict = {}\n",
    "    wordDocDictNU = {}\n",
    "    nDocs = len(texts)\n",
    "    for ii in xrange(0,nDocs):\n",
    "        textString = texts[ii].split()\n",
    "        for jj in xrange(0,len(textString)):\n",
    "            if jj == 0:\n",
    "                tempText = textString[jj][1:]\n",
    "            elif jj == len(textString)-1:\n",
    "                tempText = textString[jj][:-1]\n",
    "            else:\n",
    "                tempText = textString[jj]\n",
    "            if tempText in wordDocDict:\n",
    "                wordDocDict[tempText].add(ii)\n",
    "                wordDocDictNU[tempText].append(ii)\n",
    "            else:\n",
    "                wordDocDict[tempText] = set([ii])\n",
    "                wordDocDictNU[tempText] = [ii]\n",
    "    uniqueWords = np.array(wordDocDict.keys())\n",
    "    uniqueWordDocLabels = np.array(wordDocDict.values())\n",
    "    nuWordDocLabels = np.array(wordDocDictNU.values())\n",
    "    nDocPerWord = np.array(map(lambda x: len(x),uniqueWordDocLabels))\n",
    "    \n",
    "    # only keep words that appear in at least 5 tweets\n",
    "    uniqueWords = uniqueWords[nDocPerWord > 4]\n",
    "    nuWordDocLabels = nuWordDocLabels[nDocPerWord > 4]\n",
    "    nInstances = np.array(map(lambda x: len(x),nuWordDocLabels))\n",
    "    \n",
    "    allWords = []\n",
    "    docLabel = []\n",
    "    \n",
    "    remW = set([ii.lower() for ii in ['@', 'me', 'my', '-', 'the', 'is', 'it', 'in', 'just',\\\n",
    "     'for', 'was', 'no', 'when', 'not', 'that', 'and', 'take',\\\n",
    "     'get',  'I', 'on', 'of', 'with', 'at', 'you', 'all', 'to',\\\n",
    "     \"I'm\", 'a', \"don't\",'The', 'are', 'back', 'be', 'up', 'go',\\\n",
    "     'from', 'about', 'this', 'do', 'out', 'have',\\\n",
    "     'so', 'will', 'like', '&amp;', 'w\\\\/', 'but']])\n",
    "    \n",
    "    filter1 = np.array([True]*len(uniqueWords))\n",
    "    for ii in xrange(0,len(uniqueWords)):\n",
    "        if uniqueWords[ii] in remW:\n",
    "            filter1[ii] = False\n",
    "    \n",
    "    uniqueWords = uniqueWords[filter1]\n",
    "    nuWordDocLabels = nuWordDocLabels[filter1]\n",
    "    nInstances = nInstances[filter1]\n",
    "    \n",
    "    for ii in xrange(0,len(uniqueWords)):\n",
    "        allWords = allWords + [uniqueWords[ii]]*len(nuWordDocLabels[ii])\n",
    "        docLabel = docLabel + nuWordDocLabels[ii]\n",
    "        \n",
    "    return np.array(allWords), np.array(docLabel), uniqueWords, nInstances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = open('zipTweets_20.txt','r')\n",
    "tweets = data.read()\n",
    "data.close()\n",
    "\n",
    "tweets = tweets.split('&&&&&')[:-1]\n",
    "\n",
    "textsTest = [tweet[:-1] for tweet in tweets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "allWords31,docLabel31,u31,n31 = preProcessWords(textsTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17498\n",
      "957\n",
      "957\n",
      "3052\n",
      "3076\n"
     ]
    }
   ],
   "source": [
    "print len(allWords31)\n",
    "print len(np.unique(allWords31))\n",
    "print len(u31)\n",
    "print len(np.unique(docLabel31))\n",
    "print len(textsTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def drawTopicInit(allWords,docLabel,nTopics,alpha,beta):\n",
    "    \n",
    "    vocab = np.unique(allWords)\n",
    "    nVocab = len(vocab)\n",
    "    wordTopicCounters = {ii:collections.Counter() for ii in vocab} \n",
    "    docTopicCounters = {ii:collections.Counter() for ii in np.unique(docLabel)}\n",
    "    topicCounters = {ii:0 for ii in xrange(0,nTopics)}\n",
    "    docCounters = {ii:0 for ii in np.unique(docLabel)}\n",
    "    \n",
    "    topicVector = np.zeros(len(allWords))\n",
    "    \n",
    "    for ii in xrange(0,len(allWords)):\n",
    "        currentWord = allWords[ii]\n",
    "        currentDoc = docLabel[ii]\n",
    "        \n",
    "        probVector = np.zeros(nTopics)\n",
    "        for jj in xrange(0,nTopics):\n",
    "            probVector[jj] = (wordTopicCounters[currentWord][jj]+beta)/(topicCounters[jj] + nVocab*beta)\n",
    "            probVector[jj] = probVector[jj]*(docTopicCounters[currentDoc][jj]+alpha)/(docCounters[currentDoc] + nTopics*alpha)\n",
    "        \n",
    "        probVector = probVector/sum(probVector)\n",
    "        probCumSum = np.cumsum(probVector)\n",
    "        randInt = np.random.uniform(0,1,1)\n",
    "\n",
    "        currentTopic = sum(probCumSum < randInt)\n",
    "        topicVector[ii] = currentTopic\n",
    "        \n",
    "        wordTopicCounters[currentWord][currentTopic] += 1\n",
    "        docTopicCounters[currentDoc][currentTopic] += 1\n",
    "        topicCounters[currentTopic] += 1\n",
    "        docCounters[currentDoc] += 1  \n",
    "    \n",
    "    return topicVector, wordTopicCounters, docTopicCounters, topicCounters, docCounters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def drawTopic(wordIndex,allWords,topicLabel,docLabel,nTopics,wordTopicCounters, docTopicCounters, \\\n",
    "                                                                      topicCounters, docCounters,alpha,beta):\n",
    "    nVocab = len(wordTopicCounters.keys())\n",
    "    probVector = np.zeros(nTopics)\n",
    "    \n",
    "    currentWord = allWords[wordIndex]\n",
    "    currentDoc = docLabel[wordIndex]\n",
    "    currentTopic = topicLabel[wordIndex]\n",
    "    \n",
    "    for ii in xrange(0,nTopics):\n",
    "        if ii == currentTopic:\n",
    "            probVector[ii] = (wordTopicCounters[currentWord][ii]+beta-1)/(topicCounters[ii] + nVocab*beta-1)\n",
    "            probVector[ii] = probVector[ii]*(docTopicCounters[currentDoc][ii]+alpha-1)/(docCounters[currentDoc] + \\\n",
    "                                                                                        nTopics*alpha - 1)\n",
    "        else:\n",
    "            probVector[ii] = (wordTopicCounters[currentWord][ii]+beta)/(topicCounters[ii] + nVocab*beta)\n",
    "            probVector[ii] = probVector[ii]*(docTopicCounters[currentDoc][ii]+alpha)/(docCounters[currentDoc] + \\\n",
    "                                                                                        nTopics*alpha)\n",
    "                                                                                                                       \n",
    "    probVector = probVector/sum(probVector)\n",
    "    probCumSum = np.cumsum(probVector)\n",
    "    randInt = np.random.uniform(0,1,1)\n",
    "    \n",
    "    newTopic = sum(probCumSum < randInt)\n",
    "    \n",
    "    if newTopic != currentTopic:\n",
    "        wordTopicCounters[currentWord][newTopic] += 1\n",
    "        wordTopicCounters[currentWord][currentTopic] -= 1\n",
    "        docTopicCounters[currentDoc][newTopic] += 1\n",
    "        docTopicCounters[currentDoc][currentTopic] -= 1\n",
    "        topicCounters[newTopic] += 1\n",
    "        topicCounters[currentTopic] -= 1\n",
    "    \n",
    "    return newTopic, wordTopicCounters, docTopicCounters, topicCounters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def logGamma2(num):\n",
    "    if num < 170:\n",
    "        return np.log(gamma(num))\n",
    "    else:\n",
    "        num = num - 1\n",
    "        return np.log(np.sqrt(2*np.pi*num)) + num*np.log(num/np.exp(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getLogLik(allWords,topicLabels,nTopics,wordTopicCounters,topicCounters,beta):\n",
    "    uniqueWords = np.unique(allWords)\n",
    "    nVocab = len(uniqueWords)\n",
    "    logLik = nTopics*(logGamma2(nVocab*beta)-nVocab*logGamma2(beta))\n",
    "    for ii in xrange(0,nTopics):\n",
    "        logLik = logLik - logGamma2(topicCounters[ii]+nVocab*beta)\n",
    "        for jj in uniqueWords:\n",
    "            logLik = logLik + logGamma2(wordTopicCounters[jj][ii]+beta)\n",
    "    return logLik"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def stationaryLDA(allWords,docLabel,nTopics):\n",
    "  \n",
    "    uniqueWords = np.unique(allWords)\n",
    "    uniqueDocs = np.unique(docLabel)\n",
    "    nVocab = len(uniqueWords)\n",
    "    nDocs = len(uniqueDocs)\n",
    "    \n",
    "    theta = np.zeros((nDocs,nTopics))\n",
    "    phi = np.zeros((nTopics,nVocab))\n",
    "    \n",
    "    # just need allWords, docLabel, topicLabel\n",
    "    alpha = 0.001\n",
    "    beta = 0.01\n",
    "    \n",
    "    nIterations = 300\n",
    "    count = 0\n",
    "    \n",
    "    topicLabel, wordTopicCounters, docTopicCounters, topicCounters, docCounters = \\\n",
    "                                                                    drawTopicInit(allWords,docLabel,nTopics,alpha,beta)\n",
    "                                                                                               \n",
    "    logLik = np.zeros(nIterations+1)\n",
    "    logLik[0] = getLogLik(allWords,topicLabel,nTopics,wordTopicCounters,topicCounters,beta)\n",
    "    \n",
    "    while count < nIterations:\n",
    "        for ii in xrange(0,len(allWords)):\n",
    "            topic, wordTopicCounters, docTopicCounters, topicCounters = drawTopic(ii,allWords,\\\n",
    "                              topicLabel,docLabel,nTopics,wordTopicCounters, docTopicCounters, \\\n",
    "                                                                                  topicCounters, docCounters,alpha,beta)\n",
    "            topicLabel[ii] = topic\n",
    "        # update phi and theta here, calculate P(w|z)\n",
    "        for ii in xrange(0,nTopics):\n",
    "            sumCurr = topicCounters[ii]\n",
    "            for jj in xrange(0,nVocab):\n",
    "                phi[ii,jj] = (wordTopicCounters[uniqueWords[jj]][ii] + beta)/(sumCurr + nVocab*beta)\n",
    "        for ii in xrange(0,nDocs):\n",
    "            sumCurr = docCounters[uniqueDocs[ii]]\n",
    "            for jj in xrange(0,nTopics):\n",
    "                theta[ii,jj] = (docTopicCounters[uniqueDocs[ii]][jj] + alpha)/(sumCurr + nTopics*alpha)\n",
    "        logLik[count+1] = getLogLik(allWords,topicLabel,nTopics,wordTopicCounters,topicCounters,beta)\n",
    "        count = count + 1\n",
    "\n",
    "    return theta, phi, logLik, uniqueWords, uniqueDocs, topicLabel,\\\n",
    "                                            wordTopicCounters, topicCounters, docTopicCounters, docCounters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def findWord(wordArray,target):\n",
    "    index = []\n",
    "    for ii in xrange(0,len(wordArray)):\n",
    "        if wordArray[ii] == target:\n",
    "            index.append(ii)\n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def findWord2(words,target):\n",
    "    index = []\n",
    "    for ii in xrange(0,len(words)-len(target)):\n",
    "        if words[ii:ii+len(target)] == target:\n",
    "            index.append(ii)\n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "textsTest0 = np.copy(textsTest)\n",
    "allWords0,docLabel0,u0,n0 = preProcessWords(textsTest0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "startT = time.time()\n",
    "nTopics = 10\n",
    "theta,phi,logLikVec,uniqueWords,uniqueDocs,topicLabel,wordTopicCounters, topicCounters, docTopicCounters, docCounters = \\\n",
    "                    stationaryLDA(allWords0,docLabel0,nTopics)\n",
    "endT = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "341.496000051\n"
     ]
    }
   ],
   "source": [
    "print endT-startT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(logLikVec)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Print 10 topics here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['how' 'or' 'candidates' '#GOPDebate' 'many' 'I' 'he' '#CNNDeba' 'Trump'\n",
      " '#GOPDeba']\n",
      "['I' 'looks' '#CNNDebate' '#GOPDebate' 'his' '#CNNDeba' 'Rand' 'Trump'\n",
      " 'Paul' '#GOPDeba']\n",
      "['SHOULDER' 'debate' 'TO' '#CNNDeba' 'Trump' 'he' '#GOPDebate' '2' 'time'\n",
      " '#GOPDeba']\n",
      "['' 'A' 'an' 'Trump' 'he' 'debate' '#GOPDebate' 'I' '#CNNDeba' '#GOPDeba']\n",
      "['he' 'trump' 'debate' 'Donald' '#GOPDebate' 'Trump' 'I' '#CNNDeba'\n",
      " '#GOPDeba' '']\n",
      "['candidates' 'what' 'any' 'Trump' '#GOPDebate' '#CNNDebate' '' 'I'\n",
      " '#CNNDeba' '#GOPDeba']\n",
      "['GOP' 'wearing' 'watch' '#CNNDebate' 'we' 'Trump' 'I' '#GOPDeba'\n",
      " '#CNNDeba' '#GOPDebate']\n",
      "['debate' 'more' 'he' 'as' '#CNNDebate' 'Trump' '#GOPDebate' '#CNNDeba' 'I'\n",
      " '#GOPDeba']\n",
      "['he' 'ere' \"'m\" '#CNNDebate' 'here' 'I' '#GOPDebate' 'we' '#CNNDeba'\n",
      " '#GOPDeba']\n",
      "['arco' 'already' 'your' 'Trump' 'Cruz' 'joke' 'water' 'Rubio' '#CNNDeba'\n",
      " '#GOPDeba']\n"
     ]
    }
   ],
   "source": [
    "for ii in xrange(0,10):\n",
    "    print uniqueWords[np.argsort(phi[ii,:])[-10:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
