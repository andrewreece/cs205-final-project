{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time, json, boto3, re\n",
    "from dateutil import parser, tz\n",
    "from datetime import datetime, timedelta\n",
    "from sentiment import *\n",
    "from pyspark.sql import SQLContext, Row\n",
    "import pyspark.sql.functions as sqlfunc\n",
    "from pyspark.sql.types import *\n",
    "from tempfile import NamedTemporaryFile\n",
    "\n",
    "\n",
    "search_terms = []\n",
    "n_parts = 10\n",
    "\n",
    "def get_search_json(fname):\n",
    "    f = open(fname,'r')\n",
    "    rawdata = f.readlines()\n",
    "    f.close()\n",
    "    jdata = json.loads(rawdata[0])\n",
    "    return jdata\n",
    "\n",
    "def pool_search_terms(j):\n",
    "    ''' Short recursive routine to pull out all search terms in search-terms.json '''\n",
    "    if isinstance(j,dict):\n",
    "        for j2 in j.values():\n",
    "            pool_search_terms(j2)\n",
    "    else:\n",
    "        search_terms.extend( j )\n",
    "    return search_terms\n",
    "\n",
    "def is_cluster_running():\n",
    "    import boto3\n",
    "    client = boto3.client('emr')\n",
    "\n",
    "    ''' list_clusters() is used here to find the current cluster ID\n",
    "        WARNING: this is a little shaky, as there may be >1 clusters running in production\n",
    "                 better to search by cluster name as well as state\n",
    "    '''\n",
    "    clusters = client.list_clusters(ClusterStates=['RUNNING','WAITING','BOOTSTRAPPING'])['Clusters']\n",
    "\n",
    "    clusters_exist = len(clusters) > 0\n",
    "    if clusters_exist:\n",
    "        cid = clusters[0]['Id']\n",
    "    else:\n",
    "        cid = None\n",
    "    return clusters_exist, cid\n",
    "\n",
    "\n",
    "\n",
    "def make_json_nostream(tweet):\n",
    "    ''' Get stringified JSOn from Kafka, attempt to convert to JSON '''\n",
    "    try:\n",
    "        return json.loads(tweet.decode('utf-8'))\n",
    "    except:\n",
    "        return \"error\"+str(tweet.decode('utf-8'))\n",
    "\n",
    "\n",
    "def filter_tweets(item,terms):\n",
    "    ''' Filters out the tweets we do not want.  Filters include:\n",
    "            * No non-tweets (eg. delete commands)\n",
    "            * No retweets \n",
    "            * English language only\n",
    "            * No tweets with links\n",
    "                - We need to check both entities and media fields for this (is that true?) \n",
    "            * Matches at least one of the provided search terms '''\n",
    "    # Define regex pattern that covers all search terms\n",
    "    pattern = '|(\\s|#|@)'.join(terms)\n",
    "    return (isinstance(item,dict) and \n",
    "            ('delete' not in item.keys()) and\n",
    "            ('retweeted_status' not in item.keys())                           and \n",
    "            (item['lang']=='en')                       and\n",
    "            (len(item['entities']['urls'])==0)                   and\n",
    "            ('media' not in item['entities'].keys()) and\n",
    "            (re.search(pattern,item['text'],re.I) is not None)\n",
    "           )\n",
    "\n",
    "\n",
    "def get_relevant_fields(item,json_terms,debate_party):\n",
    "    ''' Reduce the full set of metadata down to only those we care about, including:\n",
    "            * timestamp\n",
    "            * username\n",
    "            * text of tweet \n",
    "            * hashtags\n",
    "            * geotag coordinates (if any)\n",
    "            * location (user-defined in profile, not necessarily current location)\n",
    "    '''\n",
    "    cands = json_terms['candidates'][debate_party]\n",
    "    mentioned = []\n",
    "    # loop over candidates, check if tweet mentions each one\n",
    "    for name, terms in cands.items():\n",
    "        p = '|(\\s|#|@)'.join(terms) # regex allows for # hashtag, @ mention, or blank space before term\n",
    "        rgx = re.search(p,item['text'],re.I)\n",
    "        if rgx: # if candidate-specific search term is matched\n",
    "            mentioned.append( name ) # add candidate surname to mentioned list\n",
    "    if len(mentioned) == 0: # if no candidates were mentioned specifically\n",
    "        mentioned.append( \"general\" ) # then tweet must be a general reference to the debate\n",
    "    return (item['id'], \n",
    "            {\"timestamp\":      time.strftime('%Y-%m-%d %H:%M:%S', time.strptime(item['created_at'],'%a %b %d %H:%M:%S +0000 %Y')),\n",
    "             \"username\":       item['user']['screen_name'],\n",
    "             \"text\":           item['text'].encode('utf8').decode('ascii','ignore'),\n",
    "             \"hashtags\":       [el['text'].encode('utf8').decode('ascii','ignore') for el in item['entities']['hashtags']],\n",
    "             \"first_term\":     mentioned[0],\n",
    "             \"search_terms\":   mentioned,\n",
    "             \"multiple_terms\": len(mentioned) > 1\n",
    "            }\n",
    "           )\n",
    "\n",
    "\n",
    "def make_row(d,doPrint=False):\n",
    "    tid = d[0]\n",
    "    tdata = d[1]\n",
    "\n",
    "    return Row(id             =tid,\n",
    "               username       =tdata['username'],\n",
    "               timestamp      =tdata['timestamp'],\n",
    "               hashtags       =tdata['hashtags'] if tdata['hashtags'] is not None else '',\n",
    "               text           =tdata['text'],\n",
    "               search_terms   =tdata['search_terms'],\n",
    "               multiple_terms =tdata['multiple_terms'],\n",
    "               first_term     =tdata['first_term']\n",
    "              )\n",
    "\n",
    "def process(rdd,json_terms,debate_party,domain_name='sentiment',n_parts=10):\n",
    "\n",
    "    rdd.cache()\n",
    "\n",
    "    try:\n",
    "\n",
    "        candidate_dict = {}\n",
    "        candidate_names = json_terms['candidates'][debate_party].keys()\n",
    "        candidate_names.append( 'general' )        \n",
    "\n",
    "        for candidate in candidate_names:\n",
    "            candidate_dict[candidate] =   {'party':debate_party if candidate is not 'general' else 'general',\n",
    "                                          'num_tweets':'0',\n",
    "                                          'sentiment_avg':'',\n",
    "                                          'sentiment_std':'',\n",
    "                                          'highest_sentiment_tweet':'',\n",
    "                                          'lowest_sentiment_tweet':''\n",
    "                                         }\n",
    "\n",
    "        # default settings remove words scored 4-6 on the scale (too neutral). \n",
    "        # adjust with kwarg stopval, determines 'ignore spread' out from 5. eg. default stopval = 1.0 (4-6)\n",
    "        labMT = emotionFileReader() \n",
    "\n",
    "        # Get the singleton instance of SQLContext\n",
    "        sqlContext = getSqlContextInstance(rdd.context)\n",
    "\n",
    "        schema = StructType([StructField(\"first_term\",      StringType()           ),\n",
    "                             StructField(\"hashtags\",        ArrayType(StringType())),\n",
    "                             StructField(\"id\",              IntegerType()          ),\n",
    "                             StructField(\"multiple_terms\",  BooleanType()          ),\n",
    "                             StructField(\"search_terms\",    ArrayType(StringType())),\n",
    "                             StructField(\"text\",            StringType()           ),\n",
    "                             StructField(\"timestamp\",       StringType()           ),\n",
    "                             StructField(\"username\",        StringType()           )\n",
    "                            ]\n",
    "                           )\n",
    "        # Convert RDD[String] to RDD[Row] to DataFrame\n",
    "        row_rdd = rdd.map(lambda data: make_row(data))\n",
    "        df = sqlContext.createDataFrame(row_rdd, schema)\n",
    "\n",
    "        # how many tweets per candidate per batch?\n",
    "        df2 = (df.groupBy(\"first_term\")\n",
    "                 .count()\n",
    "                 .alias('df2')\n",
    "              )\n",
    "\n",
    "        counts = (df2.map(lambda row: row.asDict() )\n",
    "                     .map(lambda row: (row['first_term'],row['count']))\n",
    "                  )\n",
    "        #print 'counts collect'\n",
    "        #print counts.collect()\n",
    "\n",
    "        cRdd = rdd.context.parallelize( candidate_names, n_parts )\n",
    "\n",
    "        def update_dict(d):\n",
    "            data = d[0]\n",
    "            data['num_tweets'] = str(d[1]) if d[1] is not None else data['num_tweets']\n",
    "            return data \n",
    "\n",
    "        tmp = (cRdd.map( lambda c: (c, candidate_dict[c]), preservesPartitioning=True )\n",
    "                               .leftOuterJoin( counts, numPartitions=n_parts )\n",
    "                               .map( lambda data: (data[0], update_dict(data[1])) )\n",
    "                               .collect()\n",
    "                    )\n",
    "        candidate_dict = { k:v for k,v in tmp }\n",
    "        # Register as table\n",
    "        df.registerTempTable(\"tweets\")\n",
    "        # loop over candidates, check if tweet mentions each candidate\n",
    "        for candidate in candidate_names:\n",
    "\n",
    "            accum = rdd.context.accumulator(0)\n",
    "\n",
    "            query = \"SELECT text FROM tweets WHERE first_term='{}'\".format(candidate)\n",
    "\n",
    "            result = sqlContext.sql(query)\n",
    "            try:\n",
    "                scored = result.map( lambda x: (emotion(x.text,labMT), x.text) ).cache()\n",
    "            except Exception,e:\n",
    "                print 'nothing in scored'\n",
    "                print str(e)\n",
    "\n",
    "            scored.foreach(lambda x: accum.add(1))\n",
    "\n",
    "            if accum.value > 0:\n",
    "                accum2 = rdd.context.accumulator(0)\n",
    "                try:\n",
    "                    scored = scored.filter(lambda score: score[0][0] is not None).cache()\n",
    "                except Exception,e:\n",
    "                    print 'nothing left after filtering out no-scores'\n",
    "\n",
    "                scored.foreach(lambda x: accum2.add(1))\n",
    "                if accum2.value > 1: # we want at least 2 tweets for highest and lowest scoring\n",
    "                    high_parts = scored.takeOrdered(1, key = lambda x: -x[0][0])[0]\n",
    "                    high_scores, high_tweet = high_parts\n",
    "                    \n",
    "                    #print 'high scores'\n",
    "                    #print high_scores\n",
    "                    high_avg = str(high_scores[0])\n",
    "                    high_tweet = high_tweet.encode('utf8').decode('ascii','ignore')\n",
    "                    \n",
    "                    low_parts  = scored.takeOrdered(1, key = lambda x:  x[0][0])[0]\n",
    "                    \n",
    "                    low_scores, low_tweet = low_parts\n",
    "                    #print 'low scores'\n",
    "                    #print low_scores\n",
    "                    low_avg = str(low_scores[0])\n",
    "                    low_tweet = low_tweet.encode('utf8').decode('ascii','ignore')\n",
    "\n",
    "                else:\n",
    "                    high_avg = low_avg = high_tweet = low_tweet = ''\n",
    "\n",
    "\n",
    "                candidate_dict[candidate]['highest_sentiment_tweet'] = '_'.join([high_avg,high_tweet])\n",
    "                candidate_dict[candidate]['lowest_sentiment_tweet']  = '_'.join([low_avg,low_tweet])  \n",
    "\n",
    "                sentiment = (result.map(lambda x: (1,x.text))\n",
    "                                        .reduceByKey(lambda x,y: ' '.join([str(x),str(y)]))\n",
    "                                        .map( lambda text: emotion(text[1],labMT) )\n",
    "                                        .collect()\n",
    "                                 )\n",
    "                try:\n",
    "                    sentiment_avg, sentiment_std = sentiment[0]\n",
    "                except Exception,e:\n",
    "                    print str(e)\n",
    "                    print 'sentiment is empty'\n",
    "\n",
    "                candidate_dict[candidate]['sentiment_avg'] = str(sentiment_avg)\n",
    "                candidate_dict[candidate]['sentiment_std'] = str(sentiment_std) \n",
    "\n",
    "        attrs = []\n",
    "        import boto3,json\n",
    "        client = boto3.client('sdb')\n",
    "\n",
    "        for cname,cdata in candidate_dict.items():\n",
    "            attrs.append( {'Name':cname,'Value':json.dumps(cdata),'Replace':True} )\n",
    "\n",
    "        try:\n",
    "            # write row of data to SDB\n",
    "            client.put_attributes(\n",
    "                DomainName= domain_name,\n",
    "                ItemName  = str(time.time()),\n",
    "                Attributes= attrs \n",
    "            ) \n",
    "        except Exception,e:\n",
    "            print 'sdb write error: {}'.format(str(e))\n",
    "\n",
    "        #rdd.foreachPartition(lambda p: write_to_db(p,level='group'))\n",
    "    except Exception, e:\n",
    "        print \n",
    "        print 'THERE IS AN ERROR!!!!'\n",
    "        print str(e)\n",
    "        print\n",
    "        pass\n",
    "\n",
    "\n",
    "def update_tz(d,dtype):\n",
    "    ''' Updates time zone for date stamp to US EST (the time zone of the debates) '''\n",
    "    def convert_timezone(item):\n",
    "        from_zone = tz.gettz('UTC')\n",
    "        to_zone = tz.gettz('America/New_York')\n",
    "        dt = parser.parse(item['timestamp'])\n",
    "        utc = dt.replace(tzinfo=from_zone)\n",
    "        return utc.astimezone(to_zone)\n",
    "    \n",
    "    if dtype == \"sql\":\n",
    "        return Row(id=d[0], time=convert_timezone(d[1]))\n",
    "    elif dtype == \"pandas\":\n",
    "        return convert_timezone(d[1])\n",
    "\n",
    "\n",
    "# From Thouis 'Ray' Jones CS205\n",
    "def quiet_logs(sc):\n",
    "    ''' Shuts down log printouts during execution '''\n",
    "    logger = sc._jvm.org.apache.log4j\n",
    "    logger.LogManager.getLogger(\"org\").setLevel(logger.Level.WARN)\n",
    "    logger.LogManager.getLogger(\"akka\").setLevel(logger.Level.WARN)\n",
    "    logger.LogManager.getLogger(\"amazonaws\").setLevel(logger.Level.WARN)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# from docs: http://spark.apache.org/docs/latest/streaming-programming-guide.html#dataframe-and-sql-operations\n",
    "def getSqlContextInstance(sparkContext):\n",
    "    ''' Lazily instantiated global instance of SQLContext '''\n",
    "    if ('sqlContextSingletonInstance' not in globals()):\n",
    "        globals()['sqlContextSingletonInstance'] = SQLContext(sparkContext)\n",
    "    return globals()['sqlContextSingletonInstance']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "codec = \"org.apache.hadoop.io.compress.GzipCodec\"\n",
    "\n",
    "party_of_debate = \"dem\"\n",
    "\n",
    "hours = [20,21,22,23]\n",
    "\n",
    "head_path = \"/Users/andrew/git-local/\"\n",
    "data_path = \"data/gardenhose/\"\n",
    "reduced_path = \"data/reduced/\"\n",
    "debate_date = \"/oct13/\"\n",
    "debate_date_numeric = \"2015-10-13-\"\n",
    "\n",
    "path = head_path + data_path + party_of_debate + debate_date + debate_date_numeric\n",
    "tail = \"*.gz\"\n",
    "\n",
    "\n",
    "search_terms = []\n",
    "search_json_fname = head_path+'search-terms.json'\n",
    "# Load nested JSON of search terms\n",
    "jdata = get_search_json(search_json_fname)\n",
    "# Collect all search terms in JSON into search_terms list\n",
    "search_terms = set(pool_search_terms(jdata))\n",
    "\n",
    "\n",
    "output = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for hour in range(19,24):\n",
    "    full_path = path + str(hour) + tail\n",
    "    rdd = sc.textFile(full_path,8)\n",
    "    filtered = (rdd.map(make_json_nostream) \n",
    "            .filter(lambda tweet: filter_tweets(tweet,search_terms))\n",
    "            .map(lambda tweet: get_relevant_fields(tweet,jdata,party_of_debate))\n",
    "            .cache()\n",
    "            )\n",
    "    output = filtered.collect()\n",
    "    fname = head_path + reduced_path + party_of_debate + debate_date + str(hour) + \".txt\"\n",
    "    f = open(fname, 'w')\n",
    "    for row in output:\n",
    "        row[1]['tweet_id'] = row[0]\n",
    "        tdata = row[1]\n",
    "        f.write(json.dumps(tdata))\n",
    "    f.close()\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
