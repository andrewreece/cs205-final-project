<!DOCTYPE html>
<html>
<head>
    <meta charset="utf-8">

    <title>Gauging Debate - Real-time Sentiment Analysis of 2016 Presidential Debates</title>

	<!-- JQUERY -->
    <script src="http://ajax.googleapis.com/ajax/libs/jquery/1.11.1/jquery.min.js"></script>

    <!-- CSS -->
    <link rel="stylesheet" href="{{ url_for('static', filename = 'css/main.css') }}">
  </head>

  <body>
	<div id="title">Gauging Debate: Tracking Sentiment for 2016 Elections</div>

    <div id="nav">
        <a class="nav" href="http://gaugingdebate.com/">
            <img class="nav-icon lighter" src="{{ url_for('static', filename = 'images/home.png') }}" />
        </a>

        <a class="nav" href="http://gaugingdebate.com/about">
        <img class="nav-icon" src="{{ url_for('static', filename = 'images/about.png') }}" />
        </a>
        
        <a class="nav" href="https://github.com/andrewreece/gauging-debate">
        <img class="nav-icon lighter" src="{{ url_for('static', filename = 'images/github.png') }}" />
        </a>
    </div>

  	<div id="container">
		<span class="header" id="intro">What is Gauging Debate?</span>

		<p>Gauging Debate is a data analytics application that tracks public sentiment, as measured by Twitter content, regarding the candidates in the 2016 US Presidential electoral cycle. </p>

		<p>Sentiment can be tracked in realtime, or for historical analysis.  On the app's <a href="http://gaugingdebate.com/">main page</a>, click "Live Tracking" for realtime analysis.  Click "Previous Debate" to analyze past events.  Currently, data from two past debates are available for review - the Sept 16 GOP debate and the Oct 13 Democratic debate.  See the Analysis section below for more on how sentiment is measured. </p>

		<p>
		<span style="color:red;">*Disclaimer*</span> 
		As a working application, Gauging Debate is still in alpha development.  That means it is functional, but there are still kinks to be worked out and features to be added before it's ready for prime time.  </p>

		<span style="color:red;font-weight:900;">Important!A Note To CS205 Grading Staff:</span>
		<p>This app automatically detects whether a cluster is serving streaming content.  There will likely not be a cluster running when you are reviewing our project for grading, as it's too costly for us to keep an on-demand dedicated cluster open indefinitely.  However, we will endeavor to keep a local streaming Spark instance open on our own machines for as much as possible over the next few days.  The app will still not find an AWS cluster when you click the "Live Streaming" option, but we've added a button after the "No cluster found" message, which says "Try Tracking Anyway". Click this and the app will read off of the data streaming from our local instance.  We'll do our best to keep the local instance up and running for the next few days, although we may need to shut it down from time to time due to machine memory constraints.</p>

		<span class="header" id="background">Background &amp; Motivation</span>

		<p>The app was developed as part of an assignment for Harvard University's CS205, "Computing Foundations for Computational Science".  Its authors are Daniel Rajchwald and <a href="http://andrewgarrettreece.com">Andrew Reece</a>.  The code for this app is entirely open-source, and can be acquired at its <a href="https://github.com/andrewreece/gauging-debate">GitHub repo</a>. </p>

		<p>The goal of the assignment was to demonstrate an advantage of distributed computing.  Speed, scalability, and compatibility are three common motivations to build a software solution with parallel processing methods.  Gauging Debate focuses on improving scalability and compatibility.  (It needs to be pretty fast, too, in order to provide data in near realtime, but the code is not specifically optimized for speed.)</p>  

			<span class="subhead">Scalability</span>
				<p>Twitter is a good example of a data source characterized by the "Three Vs" of big data: Velocity, Volume, and Variety.  Running at full volume, the Twitter stream outputs around <a href="http://www.internetlivestats.com/one-second/">7,000 tweets per second</a> - high Velocity.  Each tweet is accompanied by 26 fields of semi-structured metadata, such as GPS coordinates, user statistics, images, and URLs - lots of Variety. Including metadata, this all amounts to about a gigabyte of data every 5 minutes (at about 500K per tweet+metadata) - big Volume. Modern analytics solutions need to be able to handle data that comes with these characteristics - that was part of our challenge.</p>

				<p>It's also important to take into account the varying rate of Twitter data. Consider this chart of tweets about Hillary Clinton just before and during the October 13 2016 Democratic debate:</p>

				<img src="{{ url_for('static', filename = 'images/dem_oct13_clinton.png') }}" />

				<p>Each bar represents a 30-second period, and the y-axis shows how many new, non-retweet posts were created about Hillary Clinton in each period.  The debate actually started at 8:30pm EST (around 20:32 on the x-axis).  You can see how, at different points during the debate, there's more than an order of magnitude difference in the frequency of tweets (from about 50 to almost 800 per 30-second interval).  The Gauging Debate app needs to be able to scale from 80 to 800 (and higher), and having multiple processes computing the analyses in parallel allows for this flexibility. </p>

			<span class="subhead">Compatibility</span>

				<p>Gauging Debate is also an exercise in integration and compatibility. We wanted to build an application that fit snugly into the <a href="http://dataconomy.com/understanding-big-data-ecosystem/">"Big Data Ecosystem"</a>.  The "Big Data" part may be a debatable buzzword, but there is definitely a software ecosystem that has emerged around working with large, fast analytics solutions, and a lot of it is designed to run operations in parallel.  Much of this ecosystem's development is driven by Apache and Amazon Web Services, and you can see in the Data Pipeline section below how Gauging Debate makes good use of both these organizations' products.</p>


		 You can also review [our process journal](https://docs.google.com/document/d/1ncgcKObu8FmFr2-T6JLUhg-GArKaeCCcC7qfIMB1dbc/edit?usp=sharing) for all the step-by-step gory details.  
	 
		 
		If you want to run a local instance yourself, follow the instructions in the final report.
		  
		####Data Pipeline  
		Streaming data travels across several components in order to get from the raw Twitter stream to the app's web interface.  This is a rough diagram of how it happens, more detail below:  
		  
		#####    Twitter stream -> Kafka -> Spark Streaming -> Spark SQL -> SimpleDB -> Flask -> Web

		<b>Twitter stream</b>  
		We can access the Twitter stream through the Twitter developer's API, which provides free access to a small portion of the entire stream of tweets.  Since we were only attempting to acquire a small portion of all tweets anyway (ie. only candidate- or debate-related tweets), the app collects most (but not all) of its target tweets with this free access tier.

		<b>Kafka</b>  
		[Apache Kafka](http://kafka.apache.org/) is a distributed publish-subscribe messaging system that is built to work with the rest of the Apache ecosystem. It serves as a broker for incoming streams of data, and for outgoing requests. Spark Streaming has a native Kafka connector.  (Actually, Spark's Java and Scala versions have native connectors for the Twitter stream, but we developed in PySpark, which does not yet have this feature.)  

		<b>Spark Streaming</b>  
		[Spark Streaming](http://spark.apache.org/docs/latest/streaming-programming-guide.html) works more-or-less like normal Spark.  The main abstraction is the "DStream", but with a few I/O exceptions you can basically treat these like regular RDDs. There is a start, await, and exit sequence that is set to tell the stream when to open and close, and otherwise it's Spark as usual.  

		<b>Spark SQL</b>  
		[Spark SQL](http://spark.apache.org/docs/latest/sql-programming-guide.html) offers Pandas-y data frames and Hive query functionality on RDDs.  This is nice for conducting groupby operations when groupByKey() is not feasible.  In our case, it came in handy for grouping data by both timestamp and candidates.  It's worth noting that aggregate functions for groupby objects are still quite primitive, and (at least in PySpark) don't yet offer the degree of customization for aggregating functions that you might expect from, say, Pandas.  In fact, we used Pandas for analyzing the historical data, as it was much easier to get the data in the shape we needed.  

		<b>Simple DB</b>  
		Even though the goal was to stream live analytics, we still wanted the ability to (a) keep a buffer of recent past analysis, and (b) make it easy to access both live and historical data. This, on top of the fact that we're not Node.js experts, led us to a storage-based solution, wherein stream data is written to a database, and the front end then queries the most recent records for display.  Simple static databases don't work well with parallelized writes, so platforms like MySQL and SQLite were unavailable to us. Amazon offers a number of database options, and we chose [Simple DB](https://github.com/boto/boto3), a schema-less key-value storage system - mainly because it had a low learning overhead, and we didn't need very sophisticated querying.  SimpleDB can handle concurrent reads and writes, and we can interface with it in Python through [Boto](https://github.com/boto/boto3), which is a truly excellent module.

		<b>Flask</b>
		[Flask](http://flask.pocoo.org/) is a Python framework for serving web content. We used it to do all the heavy lifting between the backend and the web interface. It works in conjunction with [Jinja2 templating](http://flask.pocoo.org/).  Just about anything that isn't boilerplate on the website is served through Flask in one way or another.  

		<b>Web</b>   
		The front-end of [Gauging Debate](http://gaugingdebate.com) relies heavily on a handful of Javascript libraries: [jQuery](http://oboejs.com/), [D3](http://oboejs.com/), [Oboe](http://oboejs.com/), and [Plotly](https://plot.ly/javascript). jQuery and D3 are probably familiar to most readers.  <b>Oboe</b> is a great little library that collects large data requests in little bits, kind of like a pseudo-stream. We used this for the historical debate charts - the entire ~3 hours of debate data is a lot to load all at once, so we use D3 to load the first few minutes and render quickly, and in the background Oboe loads the rest.  When it finishes loading, the chart is refreshed with the full dataset.  <b>Plotly</b> is a charting library built on top of D3, which was just recently (as of Dec 2015) open-sourced.  It's great.  We initially started with [Rickshaw](http://code.shutterstock.com/rickshaw/), but that project is dead and wading through highly idiosyncratic source code with little documentation proved to be a terrible idea.  

		The main feature of the front-end is a chart of average tweet sentiment, per candidate, updated once every 30 seconds. (More on that time interval below in Analysis.)  The chart has a lot of built-in customization, including panning and zooming on both axes, error bars, and the ability to add or remove candidates. You can also save any chart view as an image file for download.  (We can't take credit for these great features, that's all Plotly.)  

		The web interface allows users to choose either streaming or historical analysis.  In order for streaming data to appear, there needs to be either an AWS cluster running which is processing realtime data, or a Spark instance on someone's local computer which is doing the same.  

		There is also an administrator dashboard which allows admins to start up Spark clusters for streaming functionality. The address of this dashboard is not public - if you're on the CS205 staff you should have received this address in an email.  

		<b>S3</b>  
		This isn't part of the data pipeline, per se, but we ended up storing almost all our configurations, settings, credentials, and scripts on [S3](https://aws.amazon.com/s3/).  This made it easy for us not to worry about file paths when switching between local and cluster instances, and it interfaces well with the AWS ecosystem.  Most, if not all, of the configuration files we use are not hosted here on GitHub, but are on S3 instead.

		####Analysis  
		This software analyzes tweets related to the 2016 US Presidential Debates.   
		It gauges the sentiment (ie. level of happiness) towards each candidate, and towards the election in general.  Sentiment is analyzed with unigram (word-by-word) averaging, using the [LabMT sentiment dictionary](http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0026752).  Unigram analysis is a relatively crude take on analyzing sentiment, as it is largely context-ignorant.  As such, you need to have a decent chunk of words (at least 1,000) before you can start to be confident that their averaged value is giving a reliable signal.  Based on analysis of past debates, we found that each candidate gets enough tweets to meet this limit every 30-60 seconds.  (The less popular candidates take longer than that.) Fast and frequent updates were also a priority here, as the whole point of a streaming analytics engine is that it continuously delivers content.  
		Taking all this into account, we decided to offer updated analysis in 30-second intervals.  That means every 30 seconds, the chart on [GaugingDebate.com](http://gaugingdebate.com) adds new sentiment scores.

		Topical content is determined using [a parallelized adaptation](http://www.datalab.uci.edu/papers/distributed_topic_modeling.pdf) of [Latent Dirichlet Allocation](http://machinelearning.wustl.edu/mlpapers/paper_files/BleiNJ03.pdf).  LDA is, if anything, more demanding than unigram sentiment analysis, in terms of the amount of content it needs to provide stable results. We discovered over the course of our work that it doesn't really work to run this algorithm in a streaming context, as it requires both (a) many words per document and (b) many separate documents. We tried clumping all tweets per candidate together as documents (many words per document, few documents), as well as treating each tweet as a document (few words per document, many documents). Neither one gave very satisfactory results, so, in the end, we provided the code (which works for both static and streaming Spark), but we removed it from the app itself.  See [our LDA report](https://docs.google.com/document/d/1L2Li_40lXpNFRJbz40idSyCXfB6yQdNHaa_MrdW0XWE/edit?usp=sharing) for details on implementation and results.  

  	</div>


	</body>
	</html>
  