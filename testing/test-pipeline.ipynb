{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import twython\n",
    "from twython import TwythonStreamer\n",
    "import re\n",
    "from requests_oauthlib import OAuth1\n",
    "import urllib\n",
    "\n",
    "import sys\n",
    "import ast\n",
    "import json\n",
    "\n",
    "import findspark\n",
    "findspark.init('/Users/andrew/Desktop/spark-1.5.1/')\n",
    "\n",
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.streaming.kafka import KafkaUtils\n",
    "import requests\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import threading\n",
    "import Queue\n",
    "import time\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "APP_KEY = \"bv6mnYBiFeEVKvPEZlg\"\n",
    "APP_SECRET = \"nQZk9Ca8qqJxc1Za07WyW0VPZ6gtAUSF3oPD5sun0\"\n",
    "OAUTH_TOKEN = \"606525030-ilOtJstbRvFCjUNMtOu8DP2HQKGWpQvmUsF6fblE\"\n",
    "OAUTH_TOKEN_SECRET = \"xSVE47qVOFxxZm1oqKwL6zwLVMWpzxCUYGmLJ6CVHR0mZ\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/andrew/Desktop/spark-1.5.1/'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "findspark.find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "config_token = OAuth1(APP_KEY,\n",
    "                      client_secret=APP_SECRET,\n",
    "                      resource_owner_key=OAUTH_TOKEN,\n",
    "                      resource_owner_secret=OAUTH_TOKEN_SECRET)\n",
    "\n",
    "config_url = 'https://stream.twitter.com/1.1/statuses/filter.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "debate%2Cgopdebate%2Ccnndebate%2Ccnbcdebate%2Cfoxbusinessdebate%2Crepublicandebate%2Cdemocraticdebate%2Ccandidates%2Cclinton%2Cclinton%27s%2Chillary%2Chillary%27s%2Chillaryclinton%2Creadyforhillary%2Csanders%2Csanders%27%2Csanders%27s%2Cbernie%2Cbernie%27s%2Cfeelthebern%2Ctrump%2Ctrump%27s%2Cdonaldtrump%2Cmakeamericagreatagain%2Ccarson%2Ccarson%27s%2Cbencarson%2Cchristie%2Cchristie%27s%2Cchrischristie%2Crand%2Crand%27s%2Crandpaul%2Crubio%2Crubio%27s%2Cmarcorubio%2Cbush%2Cbush%27s%2Cjeb%2Cjeb%27s%2Cjebbush%2Cjebcanfixit%2Ccruz%2Ccruz%27s%2Ctedcruz%2Ckaisch%2Ckaisch%27s%2Cjohnkaisch%2Cfiorina%2Cfiorina%27s%2Ccarlyfiorina%2Cjindal%2Cjindal%27s%2Cbobbyjindal%2Csantorum%2Csantorum%27s%2Cricksantorum%2Chuckabee%2Chuckabee%27s%2Cmikehuckabee\n"
     ]
    }
   ],
   "source": [
    "search_terms = np.loadtxt(\"search-terms.txt\",delimiter=\"\\n\",dtype=object)\n",
    "search_terms = ','.join(search_terms)\n",
    "search_terms = urllib.urlencode({\"track\":search_terms}).split(\"=\")[1]\n",
    "#print search_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BATCH_INTERVAL = 60  # How frequently to update (seconds)\n",
    "BLOCKSIZE = 50  # How many tweets per update\n",
    "\n",
    "\n",
    "def main():\n",
    "    threads = []\n",
    "    q = Queue.Queue()\n",
    "    # Set up spark objects and run\n",
    "    sc  = SparkContext('local[4]', 'Twitter Stream')\n",
    "    ssc = StreamingContext(sc, BATCH_INTERVAL)\n",
    "    threads.append(threading.Thread(target=spark_stream, args=(sc, ssc, q)))\n",
    "    [t.start() for t in threads]\n",
    "    \n",
    "def spark_stream(sc, ssc, q):\n",
    "    \"\"\"\n",
    "    Establish queued spark stream.\n",
    "    For a **rough** tutorial of what I'm doing here, check this unit test\n",
    "    https://github.com/databricks/spark-perf/blob/master/pyspark-tests/streaming_tests.py\n",
    "    * Essentially this establishes an empty RDD object filled with integers [0, BLOCKSIZE).\n",
    "    * We then set up our DStream object to have the default RDD be our empty RDD.\n",
    "    * Finally, we transform our DStream by applying a map to each element (remember these\n",
    "        were integers) and setting the next element to be the next element from the Twitter\n",
    "        stream.\n",
    "    * Afterwards we perform the analysis\n",
    "        1. Convert each string to a literal python object\n",
    "        2. Filter by keyword association (sentiment analysis)\n",
    "        3. Convert each object to just the coordinate tuple\n",
    "    :param sc: SparkContext\n",
    "    :param ssc: StreamingContext\n",
    "    \"\"\"\n",
    "    # Setup Stream\n",
    "    rdd = ssc.sparkContext.parallelize([0])\n",
    "    stream = ssc.queueStream([], default=rdd)\n",
    "\n",
    "    stream = stream.transform(tfunc)\n",
    "\n",
    "    '''# Analysis\n",
    "    coord_stream = stream.map(lambda line: ast.literal_eval(line)) \\\n",
    "                        .filter(filter_posts) \\\n",
    "                        .map(get_coord)\n",
    "\n",
    "    # Convert to something usable....\n",
    "    coord_stream.foreachRDD(lambda t, rdd: q.put(rdd.collect()))\n",
    "    \n",
    "    '''\n",
    "    stream.foreachRDD(lambda t, rdd: q.put(rdd.collect()))\n",
    "    # Run!\n",
    "    ssc.start()\n",
    "    ssc.awaitTermination()\n",
    "\n",
    "\n",
    "def stream_twitter_data():\n",
    "    \"\"\"\n",
    "    Only pull in tweets with location information\n",
    "    :param response: requests response object\n",
    "        This is the returned response from the GET request on the twitter endpoint\n",
    "    \"\"\"\n",
    "    data      = [('language', 'en'), ('track', search_terms), ('delimited','length')]\n",
    "    query_url = config_url + '?' + '&'.join([str(t[0]) + '=' + str(t[1]) for t in data])\n",
    "    response  = requests.get(query_url, auth=config_token, stream=True)\n",
    "    print(query_url, response) # 200 <OK>\n",
    "    count = 0\n",
    "    for line in response.iter_lines():  # Iterate over streaming tweets\n",
    "        try:\n",
    "            if count > BLOCKSIZE:\n",
    "                break\n",
    "            post     = json.loads(line.decode('utf-8'))\n",
    "            contents = [post['text'], post['user']['screen_name'], post['user']['location']]\n",
    "            count   += 1\n",
    "            yield str(contents)\n",
    "        except:\n",
    "            print(line)\n",
    "\n",
    "\n",
    "def tfunc(t, rdd):\n",
    "    \"\"\"\n",
    "    Transforming function. Converts our blank RDD to something usable\n",
    "    :param t: datetime\n",
    "    :param rdd: rdd\n",
    "        Current rdd we're mapping to\n",
    "    \"\"\"\n",
    "    return rdd.flatMap(lambda x: stream_twitter_data())\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    sys.exit(main())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
