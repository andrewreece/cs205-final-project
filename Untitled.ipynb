{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from requests_oauthlib import OAuth1\n",
    "import urllib\n",
    "\n",
    "import sys\n",
    "import ast\n",
    "import json\n",
    "\n",
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.streaming.kafka import KafkaUtils\n",
    "import requests\n",
    "\n",
    "import threading\n",
    "import Queue\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "APP_KEY = \"bv6mnYBiFeEVKvPEZlg\"\n",
    "APP_SECRET = \"nQZk9Ca8qqJxc1Za07WyW0VPZ6gtAUSF3oPD5sun0\"\n",
    "OAUTH_TOKEN = \"606525030-ilOtJstbRvFCjUNMtOu8DP2HQKGWpQvmUsF6fblE\"\n",
    "OAUTH_TOKEN_SECRET = \"xSVE47qVOFxxZm1oqKwL6zwLVMWpzxCUYGmLJ6CVHR0mZ\"\n",
    "\n",
    "config_token = OAuth1(APP_KEY,\n",
    "                      client_secret=APP_SECRET,\n",
    "                      resource_owner_key=OAUTH_TOKEN,\n",
    "                      resource_owner_secret=OAUTH_TOKEN_SECRET)\n",
    "\n",
    "config_url = 'https://stream.twitter.com/1.1/statuses/filter.json'\n",
    "\n",
    "BATCH_INTERVAL = 60  # How frequently to update (seconds)\n",
    "BLOCKSIZE = 50  # How many tweets per update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "search_terms = np.loadtxt(\"search-terms.txt\",delimiter=\"\\n\",dtype=object)\n",
    "search_terms = ','.join(search_terms)\n",
    "search_terms = urllib.urlencode({\"track\":search_terms}).split(\"=\")[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'1.5.1'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def main():\n",
    "    threads = []\n",
    "    q = Queue.Queue()\n",
    "    # Set up spark objects and run\n",
    "    sc  = SparkContext('local[4]', 'Twitter Stream')\n",
    "    ssc = StreamingContext(sc, BATCH_INTERVAL)\n",
    "    threads.append(threading.Thread(target=spark_stream, args=(sc, ssc, q)))\n",
    "    [t.start() for t in threads]\n",
    "    \n",
    "def spark_stream(sc, ssc, q):\n",
    "    \"\"\"\n",
    "    Establish queued spark stream.\n",
    "    For a **rough** tutorial of what I'm doing here, check this unit test\n",
    "    https://github.com/databricks/spark-perf/blob/master/pyspark-tests/streaming_tests.py\n",
    "    * Essentially this establishes an empty RDD object filled with integers [0, BLOCKSIZE).\n",
    "    * We then set up our DStream object to have the default RDD be our empty RDD.\n",
    "    * Finally, we transform our DStream by applying a map to each element (remember these\n",
    "        were integers) and setting the next element to be the next element from the Twitter\n",
    "        stream.\n",
    "    * Afterwards we perform the analysis\n",
    "        1. Convert each string to a literal python object\n",
    "        2. Filter by keyword association (sentiment analysis)\n",
    "        3. Convert each object to just the coordinate tuple\n",
    "    :param sc: SparkContext\n",
    "    :param ssc: StreamingContext\n",
    "    \"\"\"\n",
    "    # Setup Stream\n",
    "    rdd = ssc.sparkContext.parallelize([0])\n",
    "    stream = ssc.queueStream([], default=rdd)\n",
    "    stream = stream.transform(tfunc)\n",
    "    '''# Analysis\n",
    "    coord_stream = stream.map(lambda line: ast.literal_eval(line)) \\\n",
    "                        .filter(filter_posts) \\\n",
    "                        .map(get_coord)\n",
    "\n",
    "    # Convert to something usable....\n",
    "    coord_stream.foreachRDD(lambda t, rdd: q.put(rdd.collect()))\n",
    "    \n",
    "    '''\n",
    "    stream.foreachRDD(lambda t, rdd: q.put(rdd.collect()))\n",
    "    # Run!\n",
    "    ssc.start()\n",
    "    ssc.awaitTermination()\n",
    "\n",
    "\n",
    "def stream_twitter_data():\n",
    "    \"\"\"\n",
    "    Only pull in tweets with location information\n",
    "    :param response: requests response object\n",
    "        This is the returned response from the GET request on the twitter endpoint\n",
    "    \"\"\"\n",
    "    data      = [('language', 'en'), ('track', search_terms)]\n",
    "    query_url = config_url + '?' + '&'.join([str(t[0]) + '=' + str(t[1]) for t in data])\n",
    "    response  = requests.get(query_url, auth=config_token, stream=True)\n",
    "    print(query_url, response) # 200 <OK>\n",
    "    count = 0\n",
    "    for line in response.iter_lines():  # Iterate over streaming tweets\n",
    "        try:\n",
    "            if count > BLOCKSIZE:\n",
    "                break\n",
    "            post     = json.loads(line.decode('utf-8'))\n",
    "            contents = [post['text'], post['user']['screen_name'], post['user']['location']]\n",
    "            count   += 1\n",
    "            if not isinstance(contents,int):\n",
    "                print(str(contents))\n",
    "            yield str(contents)\n",
    "        except:\n",
    "            print(line)\n",
    "\n",
    "\n",
    "def tfunc(t, rdd):\n",
    "    \"\"\"\n",
    "    Transforming function. Converts our blank RDD to something usable\n",
    "    :param t: datetime\n",
    "    :param rdd: rdd\n",
    "        Current rdd we're mapping to\n",
    "    \"\"\"\n",
    "    return rdd.flatMap(lambda x: stream_twitter_data())\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    sys.exit(main())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
